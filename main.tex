%File: formatting-instructions-latex-2026.tex
%release 2026.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai2026}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}

% Additional packages for tables
\usepackage{booktabs}
\usepackage{multirow}

%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2026.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai2026.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{CS311 Final Report \\ 
ViLeXa: Vietnamese Legal Question Answering System with \\ Self-Reflective Retrieval-Augmented Generation}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Tran Tuan Kiet\textsuperscript{\rm 1} (23520822),
    Nguyen My Thong\textsuperscript{\rm 1}  (23521527)
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}University of Information Technology, Ho Chi Minh City, Vietnam\\
}

% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Legal question answering systems require accurate retrieval of relevant legal provisions and faithful generation of respon	Legal question answering systems require accurate retrieval of relevant legal provisions and faithful generation of responses grounded in authoritative sources. This project presents ViLeXa, a Vietnamese Legal Question Answering system that combines Retrieval-Augmented Generation (RAG) with self-reflective mechanisms to improve response quality. Our system features three key components: (1) a hierarchical chunking strategy that preserves the structural organization of Vietnamese legal documents (Phan, Chuong, Muc, Dieu), (2) a Vietnamese-optimized embedding model achieving 77.3\% Hit Rate@1 on the Zalo AI Legal Retrieval benchmark, and (3) a self-reflective RAG pipeline built with LangGraph that implements query routing, document grading, and adaptive query rewriting. Experimental results on 150 legal queries demonstrate that our traditional RAG pipeline with $k=5$ context documents achieves the best answer relevancy score of 0.818 while maintaining 91.3\% faithfulness. The self-reflective approach shows comparable quality with additional overhead, suggesting its value for complex queries requiring iterative refinement.
s block between (not within) the abstract and the main body of the paper.
\end{abstract}
\begin{links}
	\link{Code}{https://github.com/trtkiet/ViLeXa}
	% \link{Datasets}{https://aaai.org/example/datasets}
	% \link{Extended version}{https://aaai.org/example/extended-version}
\end{links}

\section{Introduction}

Access to legal information is fundamental to the rule of law, yet navigating the vast landscape of Vietnamese legislation remains a significant challenge for citizens and legal professionals alike. The legal system is comprised of an extensive array of documents—including laws, decrees, circulars, and ordinances—defined by complex hierarchical structures and specialized terminology that often make manual research both time-consuming and prone to error.

While recent breakthroughs in Large Language Models and Retrieval-Augmented Generation offer exciting potential for automated legal assistance, applying them to Vietnamese law remains complex due to the intricate structural and linguistic nature of the source material. Successfully navigating this domain requires overcoming significant hurdles related to preserving the context of highly organized documents, accurately interpreting specialized terminology, and ensuring that information retrieval systems are robust enough to handle the unique demands of legal queries.

This report details the development of ViLeXa (Vietnamese Legal Expert Assistant), a system engineered to bridge this gap. ViLeXa moves beyond standard retrieval methods by implementing a domain-specific pipeline that includes:

\begin{enumerate}
	\item Automated data collection from the Vietnamese Legal Portal\footnote{\url{https://vbpl.vn/}}.
	\item Structure-aware hierarchical chunking.
	\item Hybrid retrieval using sparse and dense embeddings optimized for Vietnamese.
	\item Two RAG architectures: a traditional pipeline and a self-reflective pipeline with query routing, document grading, and adaptive rewriting
\end{enumerate}

While the current iteration of ViLeXa demonstrates the viability of our domain-specific approach, achieving human-level legal reasoning remains an ongoing challenge. The system establishes a strong functional baseline, yet our testing reveals distinct limitations in handling edge cases and highly ambiguous queries. We present these findings not only to validate our architectural choices but also to clearly define the roadmap for the necessary optimizations required for real-world deployment.

\section{Related Works}

\subsection{Retrieval-Augmented Generation}

Retrieval-Augmented Generation (RAG) was introduced to address the knowledge limitations of large language models by grounding generation in retrieved documents \cite{lewis2020retrieval}. The standard RAG pipeline consists of three stages: indexing documents into a vector store, retrieving relevant passages given a query, and generating responses conditioned on the retrieved context. This approach has been widely adopted for knowledge-intensive tasks including question answering, fact verification, and dialogue systems.

\subsection{Agentic and Self-Reflective RAG}

Recent work has extended RAG with agentic capabilities that enable dynamic decision-making during retrieval and generation \cite{asai2023selfraglearningretrievegenerate, yan2024correctiveretrievalaugmentedgeneration}. These approaches introduce mechanisms for:
\begin{itemize}
	\item \textbf{Query Routing}: Deciding whether retrieval is necessary for a given query 
	\item \textbf{Document Grading}: Evaluating the relevance of retrieved documents before generation
	\item \textbf{Query Rewriting}: Reformulating queries when initial retrieval fails to find relevant documents
	\item \textbf{Response Validation}: Checking generated responses for hallucination and relevance
\end{itemize}

LangGraph\footnote{\url{https://www.langchain.com/langgraph}} provides a framework for building such agentic workflows as state machines, enabling complex multi-step reasoning with conditional branching.

\subsection{Vietnamese Text Embeddings}

Embedding models for Vietnamese have evolved from early multilingual models like mBERT to modern, versatile architectures. Notable advancements include gte-multilingual-base \cite{zhang2024mgte}, which serves as a lightweight and efficient option, and BGE-M3 \cite{bge-m3}, which introduced a unified framework supporting dense, sparse (lexical), and multi-vector retrieval for over 100 languages. Leveraging the latter's architecture, Vietnamese-Embedding-v2 \cite{Vietnamese_Embedding} fine-tuned BGE-M3 to specifically optimize retrieval for the Vietnamese language.

\subsection{Legal Question Answering}

Developing Legal QA systems for Vietnam requires navigating unique challenges, such as complex hierarchical document structures, extensive cross-referencing between laws, and the strict necessity for precise source citation. Recent efforts have focused on adapting general NLP architectures to the specific nuances of the Vietnamese legal domain \cite{duong2014vietnameselegalqa,ba2024vietnameselegalinformationretrieval}. Significant progress in this area has been driven by competitions such as the Zalo AI Legal Text Retrieval challenge and the VLSP shared tasks which established standard benchmark datasets for evaluating retrieval and question-answering systems on Vietnamese legal documents.

\section{Data Preparation}

\subsection{Data Collection}

We developed a web crawler to collect legal documents from the Vietnamese Legal Portal\footnote{\url{https://vbpl.vn/}}, the official government repository for legal documents. The collection process operates in two stages: first, the system performs document discovery by iterating through document type categories (IDs 15-23) and pagination to identify target URLs. Second, during the content extraction phase, we utilize the \textbf{BeautifulSoup} library to parse the downloaded HTML. Specifically, the system targets the content in paragraph elements, stripping HTML tags and formatting artifacts to convert the raw data into clean, plain text. This processed content is subsequently saved as JSON files containing the document ID and full text, organized by document type within the file system.

\begin{figure}
	\centering
	\includegraphics[width=1.0\columnwidth]{documents.png}
	\captionsetup{justification=centering}
	\caption{Count of Documents Collected by Type}
	\label{fig:doc_count}
\end{figure}

The crawler adheres to respectful scraping practices, implementing rate limiting with 2-second delays between requests, retry logic with exponential backoff, and filtering mechanisms for expired documents. The dataset encompasses a wide range of legal categories, for detailed see Figure \ref{fig:doc_count}.

\subsection{Chunking Technique} \label{sec:chunking}

Vietnamese legal documents follow a strict hierarchical structure that provides important context for understanding individual provisions. We developed a specialized chunking pipeline that:

\begin{enumerate}
	\item \textbf{Parses Document Hierarchy}: Uses regex patterns to identify structural elements:
	      \begin{itemize}
		      \item \textit{Chuong} (Chapter).
		      \item \textit{Muc} (Section).
		      \item \textit{Dieu} (Article).
	      \end{itemize}

	\item \textbf{Creates Contextual Chunks}: Each chunk is created at the article level with a context header prepended:
	      \begin{verbatim}
[CHUONG I | Dieu 1. Pham vi dieu chinh]

Dieu 1. Pham vi dieu chinh...
    \end{verbatim}

	\item \textbf{Handles Overflow}: Articles exceeding the token limit are split using recursive character splitting with overlaping windows to preserve context.
\end{enumerate}

Based on careful observation and manual inspection of the dataset, we determined the optimal parameters for our recursive splitting strategy. We selected a maximum chunk size of 512 tokens with an overlap of 50 tokens. This configuration was chosen empirically, as it accommodates the typical length of Vietnamese legal articles while ensuring sufficient context overlap to prevent information loss at chunk boundaries.

Each chunk retains rich metadata including document ID, document type, title, and hierarchical position (\textit{phan}, \textit{chuong}, \textit{muc}, \textit{dieu}) for provenance tracking.


\section{Proposed System}

\subsection{System Architecture}

\begin{figure*}
	\centering
	\includegraphics[width=0.9\textwidth]{system_architecture.png}
	\captionsetup{justification=centering}
	\caption{ViLeXa System Architecture}
	\label{fig:system_architecture}
\end{figure*}

\subsubsection{Data Ingestion}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{ingestion.jpg}
	\captionsetup{justification=centering}
	\caption{Overview of Data Ingestion Pipeline}
	\label{fig:data_ingestion}
\end{figure}

The ingestion layer forms the foundation of the system, responsible for transforming raw legal texts into a structured vector knowledge base. The process begins with a Crawler that aggregates unstructured documents from external sources, utilizing Beautiful Soup to parse HTML content into clean, plain text. These documents are passed to a chunking process which is mentioned in \ref{sec:chunking}, which segments the text into semantically meaningful units to fit within model context windows while preserving local context. Subsequently, an embedding model transforms these text chunks into high-dimensional vector representations, capturing semantic nuances. Finally, these vectors, along with their metadata, are indexed in Qdrant, a vector database optimized for high-performance similarity search. To have an overview of this process, please refer to Figure \ref{fig:data_ingestion}.

\subsubsection{Retrieval}
The retrieval engine connects user intent with relevant knowledge. When a query is received, it is first processed by the {Query Embedding} module, which maps the input text into the same latent space as the document corpus. A {Vector Search} is then executed to identify the nearest neighbor chunks based on a similarity metric (specifically Cosine Similarity). We can also optionally utilize Hybrid Search, combining dense semantic retrieval with sparse keyword matching to leverage both deep semantic understanding and exact term precision. To further refine precision, an {Optional Reranking} stage may be applied, where a cross-encoder model re-scores the initial set of candidates to filter out false positives and prioritize the most semantically relevant passages.

\subsubsection{Generation}
The generation module synthesizes the final output using a Retrieval-Augmented Generation (RAG) approach. The process starts with {Context Assembly}, where the top-ranked retrieved passages are concatenated and formatted. This context is then integrated into a {Prompt Construction} template, which instructs the Large Language Model (LLM) to answer the user's query strictly based on the provided information. The {LLM Response} is then generated, ensuring the output is grounded in the retrieved legal text rather than relying solely on the model's parametric memory.

\subsubsection{Serving}
The serving layer acts as the interface between the core logic and the end-user. A FastAPI Backend manages incoming HTTP requests, orchestrates the workflow between the retrieval and generation components, and handles concurrency. The final results are delivered to a Chat Interface built with React, which provides a user-friendly environment for interaction, displaying both the generated answer and citations to the source documents for verification.

\subsection{Embedding and Retrieval Strategy}

To determine the optimal embedding model for our system, we evaluated three candidate models: gte-multilingual-base, bge-m3, and Vietnamese-Embedding-v2.

A significant architectural distinction exists between these candidates. Both gte-multilingual-base and bge-m3 are capable of generating both dense and sparse embeddings, which allows for a hybrid retrieval strategy combining semantic and lexical matching. In contrast, Vietnamese-Embedding-v2 is a specialized model designed to generate only dense embeddings.

Despite the lack of native sparse support, our experiments (detailed in Section \ref{sec:experiments}) demonstrated that Vietnamese-Embedding-v2 outperformed the hybrid configurations of the other models on our dataset. Consequently, we selected it for our final architecture.

The resulting retrieval pipeline is indexed in the Qdrant vector database and operates as follows:

\begin{itemize}
    \item Embedding: Input queries and documents are encoded into 1024-dimensional dense vectors.
    \item Retrieval: We employ a Dense Retrieval strategy using Cosine Similarity to identify the most semantically relevant documents.
\end{itemize}

To prioritize inference speed, we implement this as a single-stage pipeline. We rely solely on the high-quality dense representations from the fine-tuned model and do not employ a secondary reranker or cross-encoder.

\subsection{Traditional RAG Pipeline}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{traditional_rag.png}
	\captionsetup{justification=centering}
	\caption{Traditional RAG Pipeline}
	\label{fig:traditional_rag}
\end{figure}

The baseline system for our study implements a standard, linear Retrieval-Augmented Generation (RAG) workflow. This approach follows a strict retrieve-then-generate sequence without intermediate reasoning steps. The process consists of three distinct phases:

\begin{enumerate}
    \item Retrieval: The user's input query is embedded into a vector space, and the system retrieves the top-k most similar documents from the Qdrant database using the dense retrieval strategy described previously.
    \item Context Construction: The retrieved documents are concatenated directly to form the context window, without filtering for relevance.
    \item Generation: This context is passed to the Large Language Model (Gemini 2.5 Flash Lite) along with a system prompt. The prompt strictly instructs the model to act as a Vietnamese legal expert, answer solely based on the provided information, and explicitly admit ignorance if the context is insufficient.
\end{enumerate}

While effective for direct questions with clear keywords, this linear pipeline lacks flexibility. It processes every input through the retrieval engine regardless of intent and cannot recover from initial retrieval failures caused by ambiguous or poorly phrased queries.

\subsection{Self-Reflective RAG Pipeline}

To address the limitations of the linear approach, we developed a Self-Reflective RAG architecture orchestrated using LangGraph. This system introduces agentic behaviors that allow the pipeline to critique its own retrieval results and adapt its strategy dynamically. This architecture is specifically designed to resolve common failure modes such as handling conversational chit-chat, clarifying ambiguous queries, and filtering irrelevant context.

The pipeline consists of several intelligent decision-making nodes:

\begin{enumerate}
    \item Query Routing: To optimize resource usage and user experience, an initial LLM call analyzes the user's intent. It distinguishes between general conversational inputs (e.g., greetings, small talk) and domain-specific legal inquiries. "Chit-chat" queries bypass the retrieval process entirely and are answered directly, preventing unnecessary database operations and ensuring natural conversation flow. Furthermore, this mechanism prevents the unnecessary citation of law documents for normal user queries, ensuring that legal references are provided only when actually requested.

    \item Document Retrieval: For legal queries, the system executes the dense retrieval process to fetch candidate documents from the vector store.

    \item Document Grading: Unlike the traditional pipeline which blindly trusts retrieval results, the self-reflective system includes a grading node. Each retrieved document is evaluated by the LLM for relevance to the specific question. Documents deemed irrelevant are filtered out before reaching the generation phase, reducing the risk of hallucination caused by unrelated context.

    \item Query Rewriting and Refinement loop: The system employs a cyclical feedback mechanism to handle ambiguity. If the document grader finds no relevant documents—indicating that the initial query may have been vague or misaligned with the database terminology—the system does not give up. Instead, it triggers a query rewriter. The LLM reformulates the original question using more precise legal terminology and executes a new retrieval attempt.
\end{enumerate}

This adaptive loop repeats up to three times. If relevant context is found, the system generates a grounded response. If the maximum retries are exceeded without success, the system creates a graceful failure response, ensuring the user is informed of the limitation rather than receiving incorrect information.

\begin{figure*}
	\centering
	\includegraphics[width=0.9\textwidth]{self_reflective.png}
	\captionsetup{justification=centering}
	\caption{Self-Reflective RAG Workflow}
	\label{fig:self_reflective_rag}
\end{figure*}

\section{Experiments} \label{sec:experiments}

We evaluate ViLeXa on two dimensions: retrieval quality and generation quality.

\subsection{Retrieval Evaluation}

\subsubsection{Dataset and Preprocessing}

We evaluate our system using the test split of the \textit{ZacLegalTextRetrieval} dataset from the MTEB benchmark (sourced from \textit{GreenNode/zalo-ai-legal-text-retrieval-vn}). The corpus consists of 61,425 legal documents, pre-segmented at the Article (Dieu) level, and contains 793 annotated queries.

To ensure effective retrieval across varying document lengths, we implemented a sliding window chunking strategy. Each document was divided into chunks with a maximum size of 512 tokens and an overlap of 50 tokens.

The evaluation follows a strict retrieval-only protocol without a generation phase:
\begin{enumerate}
    \item \textit{Ingestion}: All document chunks are embedded and indexed in the Qdrant vector database.
    \item \textit{Retrieval}: For each query, we retrieve the top-$k$ most relevant chunks.
    \item \textit{Mapping}: To align with the ground truth annotations, the retrieved chunks are mapped back to their original corpus IDs (Article IDs).
    \item \textit{Scoring}: Evaluation metrics are calculated based on these resolved unique document IDs against the standard qrels provided in the dataset.
\end{enumerate}

\subsubsection{Models and Metrics}

We compare three embedding models on this benchmark:
\begin{itemize}
    \item GTE-Multilingual-Base (Alibaba-NLP): 768-dimensional multilingual embeddings.
    \item BGE-M3 (BAAI): 1024-dimensional multilingual embeddings with native sparse support.
    \item Vietnamese-Embedding-v2 (AITeamVN): A version of BGE-M3 fine-tuned specifically for the Vietnamese language.
\end{itemize}

For each model, we evaluate dense, sparse, and hybrid (dense + sparse) retrieval modes where applicable. Performance is measured using Hit Rate, Precision, Recall, and F1 Score at $k \in \{1, 5, 10\}$.

\subsubsection{Results}

\begin{table}[ht]
	\centering
	\captionsetup{justification=centering}
	\caption{Performance comparison of retrieval models on Zalo AI Legal Retrieval benchmark.}
	\label{tab:competitor_comparison_wide}

	\resizebox{\columnwidth}{!}{%
		\setlength{\tabcolsep}{3pt}
		\begin{tabular}{l|ccc|ccc}
			\toprule
			\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c|}{\textbf{Hit Rate}} & \multicolumn{3}{c}{\textbf{F1 Score}}                                                                     \\
			                                & @1                                     & @5                                    & @10            & @1             & @5             & @10            \\
			\midrule
			GTE Dense                       & 0.513                                  & 0.827                                 & 0.873          & 0.513          & 0.276          & 0.159          \\
			GTE Sparse                      & 0.440                                  & 0.720                                 & 0.800          & 0.440          & 0.240          & 0.146          \\
			GTE Hybrid                      & 0.527                                  & 0.793                                 & 0.880          & 0.527          & 0.264          & 0.160          \\
			\midrule
			BGE-M3 Dense                    & 0.547                                  & 0.847                                 & 0.867          & 0.547          & 0.282          & 0.158          \\
			BGE-M3 Sparse                   & 0.493                                  & 0.793                                 & 0.873          & 0.493          & 0.264          & 0.159          \\
			BGE-M3 Hybrid                   & 0.580                                  & 0.820                                 & 0.900          & 0.580          & 0.273          & 0.164          \\
			\midrule
			Viet-Embed Dense                & \textbf{0.773}                         & \textbf{0.947}                        & \textbf{0.953} & \textbf{0.773} & \textbf{0.315} & \textbf{0.173} \\
			\bottomrule
		\end{tabular}%
	}
\end{table}

Table~\ref{tab:competitor_comparison_wide} summarizes the performance of various retrieval models on the Zalo AI Legal Retrieval benchmark. The results demonstrate that the {Viet-Embed Dense} model significantly outperforms all competitor configurations, achieving a Hit Rate@1 of 0.773. This represents a substantial improvement over the strongest baseline, BGE-M3 Hybrid, which scored 0.580. While hybrid architectures generally enhanced the performance of the multilingual baselines (GTE and BGE-M3) compared to their individual sparse or dense components, they ultimately failed to bridge the gap with the specialized model. This evident disparity highlights the superior efficacy of language-specific fine-tuning over general multilingual approaches for legal domain retrieval.

\subsubsection{Reranker Analysis}

We also evaluated the impact of cross-encoder reranking on retrieval quality:

\begin{table}[ht]
	\centering
	\captionsetup{justification=centering}
	\caption{Impact of reranking on Vietnamese-Embedding retrieval (Retrieval $K$ = 10).}
	\label{tab:reranker}

	\resizebox{\columnwidth}{!}{%
		\begin{tabular}{l|ccc|c}
			\toprule
			\textbf{Configuration} & \textbf{Hit@1} & \textbf{Hit@3} & \textbf{Hit@5} & \textbf{Runtime (s)} \\
			\midrule
			Dense Only             & {0.773} & {0.900} & {0.947} & \textbf{0.13}        \\
			+ bge-v2-m3 Reranker   & 0.673          & 0.853          & 0.940          & 22.44                \\
			+ gte-multi Reranker   & 0.520          & 0.867          & 0.913          & 8.36                 \\
			+ Reranker (Vietnamese-Reranker) & \bf 0.8267 & \bf 0.9067 & \bf 0.9467 & 23.22 \\
			\bottomrule
		\end{tabular}%
	}
\end{table}

The quantitative results of our evaluation are presented in Table \ref{tab:reranker}. Notably, the results in the final row demonstrate that our single-stage dense retrieval using Vietnamese-Embedding-v2 outperforms the pipeline equipped with the baseline reranker model. While the fine-tuned Vietnamese-Reranker achieved marginally higher metrics, the performance gain was insufficient to justify the significant increase in latency. Furthermore, the generic baseline rerankers degraded performance compared to the pure dense retrieval. Consequently, we opted to exclude the reranker stage entirely, prioritizing system efficiency without compromising retrieval quality.

\subsection{Generation Evaluation}

\subsubsection{Experimental Setup}

We evaluate the generation quality of our proposed approach on the multiple-choice question task from the \textbf{VLSP2025 Public-test} dataset. This task assesses factual knowledge and comprehension of Vietnamese legal documents. Each entry consists of a question, a list of choices, and the correct answer. By extracting the correct choice for each question, we constructed a dataset of 146 question-answer pairs for evaluation.

We compare four configurations using \textbf{Gemini 2.5 Flash Lite} as the base generator:
\begin{itemize}
    \item \textbf{Traditional RAG}: Evaluated with retrieval depths of $k=3$ and $k=5$ context documents.
    \item \textbf{Self-Reflective RAG}: Evaluated with retrieval depths of $k=3$ and $k=5$ context documents.
\end{itemize}

To quantify performance, we employ the {DeepEval} framework---an LLM-as-a-judge evaluation suite---to assess two key metrics:
\begin{itemize}
    \item \textbf{Answer Relevancy}: Measures how accurately the response addresses the user's query.
    \item \textbf{Faithfulness}: Assesses whether the response is grounded in the retrieved context, ensuring the absence of hallucinations.
\end{itemize}

\subsubsection{Results}

\begin{table}[htbp]
	\centering
	\caption{Performance summary of RAG pipelines.}
	\label{tab:rag_summary}
	\begin{tabular}{lcccc}
		\toprule
		\textbf{Architecture} & \textbf{Runtime} & \textbf{Tokens} & \textbf{Rel.}  & \textbf{Faith.} \\
		\midrule
		Self-Ref. $k=3$    & 6.62s            & 4069            & 0.784          & 0.912           \\
		Self-Ref. $k=5$    & 8.06s            & 5736            & 0.785          & 0.902           \\
		Trad. $k=3$     & 1.81s            & \textbf{1550}            & 0.781          & 0.904           \\
		Trad. $k=5$     & \textbf{3.05s}   & 2472            & \textbf{0.818} & \textbf{0.913}  \\
		\bottomrule
	\end{tabular}
\end{table}

Table~\ref{tab:rag_summary} presents the performance trade-offs between the Traditional and Self-Reflective RAG pipelines. Quantitatively, the Traditional RAG ($k=5$) configuration achieves the highest metrics, recording 0.818 for Relevancy and 0.913 for Faithfulness with superior runtime efficiency. While the Self-Reflective architecture incurs higher computational overhead and yields slightly lower scores, the performance gap is marginal and arguably within the variance of the LLM-as-a-judge evaluation method. Crucially, despite the raw numerical difference, the Self-Reflective approach maintains high generation quality while offering a significant qualitative advantage: unlike the Traditional baseline, it successfully resolves complex interaction scenarios such as chitchat and ambiguous queries, justifying the additional latency for improved system robustness.

\section{Future Work}

Several directions remain for improving ViLeXa:

\begin{itemize}
    \item \textbf{Citation Generation}: Automatically generating proper legal citations (e.g., ``Dieu 15, Luat Doanh nghiep 2020'') would increase the practical utility of responses.

    \item \textbf{Temporal Reasoning}: Legal documents have effective dates and may be superseded by newer versions. Incorporating temporal metadata into retrieval could ensure users receive current legal information.

    \item \textbf{Dynamic Document Management}: Implementing an automated pipeline to manage the document lifecycle is essential for legal accuracy. This system would update the knowledge base in real-time by indexing new regulations as they become effective and flagging or archiving expired documents to ensure the database remains current.

    \item \textbf{Advanced Architectures \& Agent Integration}: To further enhance system robustness, future work will explore alternative frameworks such as Self-RAG or Corrective RAG. Furthermore, integrating autonomous agents with internet search capabilities will extend the system's reach beyond static databases to handle real-time information or external queries.

    \item \textbf{User Feedback Integration}: Collecting user feedback on response quality could enable continuous improvement through reinforcement learning or fine-tuning.
\end{itemize}

\section{Conclusion}

In this work, we presented ViLeXa, a specialized Question Answering system designed to navigate the linguistic and structural complexities of Vietnamese legal documents. By integrating a comprehensive data ingestion pipeline, structure-aware chunking, and a high-performance retrieval engine, we established a robust baseline for automated legal assistance.

Our evaluation highlights two key findings. First, language-specific optimization is superior to general architectural complexity in retrieval tasks. The specialized Vietnamese embedding model consistently outperformed broader multilingual baselines and hybrid configurations. This confirms that for low-resource or highly specific domains, utilizing domain-adapted dense embeddings is more effective than relying on complex sparse-dense hybrid approaches.

Second, we observed a distinct trade-off between raw performance metrics and system flexibility. While the Traditional RAG pipeline yielded the highest quantitative scores in relevancy and faithfulness with better latency, the Self-Reflective architecture demonstrated superior adaptability. Through mechanisms like query routing and self-correction, the Self-Reflective system successfully manages conversational inputs and ambiguous queries, addressing limitations inherent in linear workflows.

Ultimately, ViLeXa illustrates that while current retrieval architectures are capable of high-fidelity legal assistance, the future of autonomous legal systems lies in balancing the efficiency of standard retrieval with the adaptive reasoning capabilities of agentic workflows.


\bibliography{aaai2026}

\end{document}
